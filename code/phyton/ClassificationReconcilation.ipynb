{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1WHiPsU03GH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2937,
     "status": "ok",
     "timestamp": 1742907255512,
     "user": {
      "displayName": "V Naveen Kumar",
      "userId": "13422069018944725717"
     },
     "user_tz": -330
    },
    "id": "Hf5fp12c0-s3",
    "outputId": "0518ac36-4c1c-4943-9659-dcb99c695139"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 16:41:03,095 - INFO - \n",
      "=== STEP 1: LOADING DATA ===\n",
      "2025-03-26 16:41:03,101 - INFO - Loading data from Catalyst_History_Reconciliation.csv and Catalyst_Reconciliation.csv\n",
      "2025-03-26 16:41:03,211 - INFO - Data loaded successfully. History: 20 rows, Recon: 10 rows\n",
      "2025-03-26 16:41:03,213 - INFO - \n",
      "=== STEP 2: PREPROCESSING DATA ===\n",
      "2025-03-26 16:41:03,214 - INFO - Starting data preprocessing\n",
      "C:\\Users\\Naveen\\AppData\\Local\\Temp\\ipykernel_9000\\628694277.py:85: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "2025-03-26 16:41:03,224 - INFO - Data preprocessing completed\n",
      "2025-03-26 16:41:03,225 - INFO - \n",
      "=== STEP 3: TRAINING MODEL ===\n",
      "2025-03-26 16:41:03,227 - INFO - Starting model training\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "2025-03-26 16:41:03,496 - INFO - Model training completed successfully\n",
      "2025-03-26 16:41:03,497 - INFO - \n",
      "=== STEP 4: DETECTING ANOMALIES ===\n",
      "2025-03-26 16:41:03,498 - INFO - Starting anomaly detection\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "2025-03-26 16:41:03,551 - INFO - Detected 2 anomalies\n",
      "2025-03-26 16:41:03,551 - INFO - \n",
      "=== STEP 5: GENERATING REPORT ===\n",
      "2025-03-26 16:41:03,553 - INFO - Generating anomaly report\n",
      "2025-03-26 16:41:03,556 - INFO - \n",
      "Sample of anomaly report:\n",
      "2025-03-26 16:41:03,564 - INFO -      TRADEID   RISKDATE DESKNAME           TRADING_UNIT_NAME  MatchStatus  QUANTITYDIFFERENCE  PRICEDIFFERENCE  ORIGINALFACEDIFFERENCE                                                                                                                                          Anomaly_Reason                                                                                                                                                                             COMMENT  Anomaly_Score\n",
      "6   68710014 2025-03-05   Agency  Liquid Products - Agencies  Impact_Only           -71000000         -100.086               -71000000                       Qty delta 71000000 > tolerance 1 | Price delta 100.086 > tolerance 0.001 | Original face diff -71000000 | Unusual comment pattern  [{\"blotter code updated directly in impact causing a false break, due to an additional line item for cxl\\/correct only in impact. systems are in sync and no action is needed. \"}]      -0.000541\n",
      "26  68710014 2025-03-05   Agency  Liquid Products - Agencies  Impact_Only           -71000000         -100.086               -71000000  Qty delta 71000000 > tolerance 1 | Price delta 100.086 > tolerance 0.001 | Original face diff -71000000 | Old trade (8 days) | Unusual comment pattern  [{\"blotter code updated directly in impact causing a false break, due to an additional line item for cxl\\/correct only in impact. systems are in sync and no action is needed. \"}]      -0.000541\n",
      "2025-03-26 16:41:03,568 - INFO - Saved anomaly report to detected_anomalies_report.csv\n",
      "2025-03-26 16:41:03,569 - INFO - \n",
      "=== STEP 6: FIXING ANOMALIES ===\n",
      "2025-03-26 16:41:03,570 - INFO - \n",
      "Starting anomaly fixing process\n",
      "2025-03-26 16:41:03,573 - INFO - \n",
      "=== Fixing Trade ID: 68710014 ===\n",
      "2025-03-26 16:41:03,574 - INFO - Action Type: BLOTTER_CODE_ADJUSTMENT\n",
      "2025-03-26 16:41:03,577 - INFO - Reason: Qty delta 71000000 > tolerance 1 | Price delta 100.086 > tolerance 0.001 | Original face diff -71000000 | Unusual comment pattern\n",
      "2025-03-26 16:41:03,578 - INFO - Detailed Changes:\n",
      "2025-03-26 16:41:03,583 - INFO - \n",
      "=== Fixing Trade ID: 68710014 ===\n",
      "2025-03-26 16:41:03,584 - INFO - Action Type: BLOTTER_CODE_ADJUSTMENT\n",
      "2025-03-26 16:41:03,585 - INFO - Reason: Qty delta 71000000 > tolerance 1 | Price delta 100.086 > tolerance 0.001 | Original face diff -71000000 | Old trade (8 days) | Unusual comment pattern\n",
      "2025-03-26 16:41:03,586 - INFO - Detailed Changes:\n",
      "2025-03-26 16:41:03,589 - INFO - \n",
      "=== FIXING SUMMARY ===\n",
      "2025-03-26 16:41:03,590 - INFO - Total anomalies processed: 2\n",
      "2025-03-26 16:41:03,593 - INFO - Total trades modified: 2\n",
      "2025-03-26 16:41:03,594 - INFO -   BLOTTER_CODE_ADJUSTMENT: 2 trades\n",
      "2025-03-26 16:41:03,602 - INFO - \n",
      "=== DETAILED FIX STATISTICS ===\n",
      "2025-03-26 16:41:03,603 - INFO - Total fixes applied: 2\n",
      "2025-03-26 16:41:03,604 - INFO - \n",
      "Actions applied:\n",
      "2025-03-26 16:41:03,606 - INFO -   BLOTTER_CODE_ADJUSTMENT: 2\n",
      "2025-03-26 16:41:03,607 - INFO - \n",
      "Desks affected:\n",
      "2025-03-26 16:41:03,608 - INFO -   Agency: 2\n",
      "2025-03-26 16:41:03,613 - INFO - \n",
      "Saved fixed data to updated_Catalyst_History_Reconciliation.csv\n",
      "2025-03-26 16:41:03,616 - INFO - \n",
      "=== STEP 7: VALIDATING FIXES ===\n",
      "2025-03-26 16:41:03,619 - INFO - \n",
      "Validating fixes...\n",
      "2025-03-26 16:41:03,620 - INFO - Row count consistent\n",
      "2025-03-26 16:41:03,623 - INFO - Anomaly count: before 0, after 0\n",
      "2025-03-26 16:41:03,625 - INFO - Applied 0 rounding fixes\n",
      "2025-03-26 16:41:03,628 - INFO - Validation completed\n",
      "2025-03-26 16:41:03,630 - INFO - \n",
      "=== PROCESS COMPLETED SUCCESSFULLY ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging to only print to console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "class ReconciliationAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.encoder = None\n",
    "        \n",
    "    def safe_read_csv(self, path, dtype_columns=None):\n",
    "        \"\"\"Safely read CSV with optional dtype conversion\"\"\"\n",
    "        try:\n",
    "            if dtype_columns:\n",
    "                temp_df = pd.read_csv(path, nrows=1)\n",
    "                valid_columns = {col: dtype for col, dtype in dtype_columns.items() \n",
    "                                if col in temp_df.columns}\n",
    "                return pd.read_csv(path, dtype=valid_columns or None)\n",
    "            return pd.read_csv(path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading {path}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_data(self, history_path, recon_path):\n",
    "        \"\"\"Load and merge datasets with validation\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Loading data from {history_path} and {recon_path}\")\n",
    "            \n",
    "            history_df = self.safe_read_csv(history_path, {'TRADEID': str})\n",
    "            recon_df = self.safe_read_csv(recon_path, {'TRADEID': str})\n",
    "            \n",
    "            required_cols = ['TRADEID', 'RISKDATE', 'DESKNAME', 'MatchStatus', \n",
    "                           'QUANTITYDIFFERENCE', 'PRICEDIFFERENCE', 'COMMENT']\n",
    "            for col in required_cols:\n",
    "                if col not in history_df.columns or col not in recon_df.columns:\n",
    "                    raise ValueError(f\"Missing required column: {col}\")\n",
    "            \n",
    "            if 'Anomaly' not in history_df.columns:\n",
    "                history_df['Anomaly'] = False\n",
    "                \n",
    "            combined_df = pd.concat([history_df, recon_df], ignore_index=True)\n",
    "            \n",
    "            logging.info(f\"Data loaded successfully. History: {len(history_df)} rows, Recon: {len(recon_df)} rows\")\n",
    "            return history_df, recon_df, combined_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data loading failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"Clean and transform data for modeling\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Starting data preprocessing\")\n",
    "            \n",
    "            date_cols = ['RISKDATE', 'RECONDATE', 'TRADE_DATE', 'SETTLE_DATE']\n",
    "            for col in date_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_datetime(df[col], format='%d-%m-%Y', errors='coerce')\n",
    "                    null_dates = df[col].isnull().sum()\n",
    "                    if null_dates > 0:\n",
    "                        logging.warning(f\"{null_dates} null values found in {col}\")\n",
    "            \n",
    "            if 'RECONDATE' in df.columns and 'TRADE_DATE' in df.columns:\n",
    "                df['TRADE_AGE_DAYS'] = (df['RECONDATE'] - df['TRADE_DATE']).dt.days\n",
    "            if 'RECONDATE' in df.columns and 'SETTLE_DATE' in df.columns:\n",
    "                df['SETTLE_AGE_DAYS'] = (df['RECONDATE'] - df['SETTLE_DATE']).dt.days\n",
    "            \n",
    "            numeric_cols = ['QUANTITYDIFFERENCE', 'PRICEDIFFERENCE', 'ORIGINALFACEDIFFERENCE']\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    df[col].fillna(0, inplace=True)\n",
    "            \n",
    "            if 'QUANTITYDIFFERENCE' in df.columns:\n",
    "                df['ABS_QUANTITY_DIFF'] = abs(df['QUANTITYDIFFERENCE'])\n",
    "            if 'PRICEDIFFERENCE' in df.columns:\n",
    "                df['ABS_PRICE_DIFF'] = abs(df['PRICEDIFFERENCE'])\n",
    "            if 'ORIGINALFACEDIFFERENCE' in df.columns:\n",
    "                df['ABS_OF_DIFF'] = abs(df['ORIGINALFACEDIFFERENCE'])\n",
    "            \n",
    "            if 'COMMENT' in df.columns:\n",
    "                df['COMMENT_LENGTH'] = df['COMMENT'].apply(lambda x: len(str(x)))\n",
    "                df['HAS_ROUNDING_NOTE'] = df['COMMENT'].str.contains('rounding', case=False, na=False)\n",
    "            \n",
    "            logging.info(\"Data preprocessing completed\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data preprocessing failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train_model(self, df):\n",
    "        \"\"\"Train Isolation Forest model with feature engineering\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Starting model training\")\n",
    "            \n",
    "            numerical_features = [\n",
    "                'QUANTITYDIFFERENCE', 'PRICEDIFFERENCE', 'ORIGINALFACEDIFFERENCE',\n",
    "                'TRADE_AGE_DAYS', 'SETTLE_AGE_DAYS', 'ABS_QUANTITY_DIFF',\n",
    "                'ABS_PRICE_DIFF', 'ABS_OF_DIFF', 'COMMENT_LENGTH'\n",
    "            ]\n",
    "            numerical_features = [f for f in numerical_features if f in df.columns]\n",
    "            \n",
    "            categorical_features = ['DESKNAME', 'BUY_SELL', 'TRADING_UNIT_NAME', 'MatchStatus']\n",
    "            categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "            \n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', StandardScaler(), numerical_features),\n",
    "                    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "                ],\n",
    "                remainder='drop'\n",
    "            )\n",
    "            \n",
    "            self.model = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', IsolationForest(\n",
    "                    n_estimators=150,\n",
    "                    contamination=0.05,\n",
    "                    random_state=42,\n",
    "                    verbose=1\n",
    "                ))\n",
    "            ])\n",
    "            \n",
    "            X_train = df[df['Anomaly'] == False]\n",
    "            self.model.fit(X_train)\n",
    "            \n",
    "            logging.info(\"Model training completed successfully\")\n",
    "            return self.model\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Model training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def detect_anomalies(self, df):\n",
    "        \"\"\"Detect anomalies and generate report\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Starting anomaly detection\")\n",
    "            \n",
    "            if not self.model:\n",
    "                raise ValueError(\"Model not trained. Call train_model() first.\")\n",
    "            \n",
    "            predictions = self.model.predict(df)\n",
    "            df['Detected_Anomaly'] = predictions == -1\n",
    "            df['Anomaly_Score'] = self.model.decision_function(df)\n",
    "            \n",
    "            anomalies = df[df['Detected_Anomaly']].copy()\n",
    "            \n",
    "            if not anomalies.empty:\n",
    "                anomalies['Anomaly_Reason'] = anomalies.apply(self._get_anomaly_reason, axis=1)\n",
    "            else:\n",
    "                anomalies['Anomaly_Reason'] = \"\"\n",
    "            \n",
    "            logging.info(f\"Detected {len(anomalies)} anomalies\")\n",
    "            return df, anomalies\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Anomaly detection failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _get_anomaly_reason(self, row):\n",
    "        \"\"\"Generate human-readable anomaly reason\"\"\"\n",
    "        reasons = []\n",
    "        \n",
    "        if 'QUANTITYDIFFERENCE' in row and 'QUANTITY_TOLERANCE' in row:\n",
    "            qty_diff = abs(row['QUANTITYDIFFERENCE'])\n",
    "            qty_tol = row['QUANTITY_TOLERANCE']\n",
    "            if qty_diff > qty_tol:\n",
    "                reasons.append(f\"Qty delta {qty_diff} > tolerance {qty_tol}\")\n",
    "        \n",
    "        if 'PRICEDIFFERENCE' in row and 'PRICE_TOLERANCE' in row:\n",
    "            price_diff = abs(row['PRICEDIFFERENCE'])\n",
    "            price_tol = row['PRICE_TOLERANCE']\n",
    "            if price_diff > price_tol:\n",
    "                reasons.append(f\"Price delta {price_diff} > tolerance {price_tol}\")\n",
    "        \n",
    "        if 'ORIGINALFACEDIFFERENCE' in row and row['ORIGINALFACEDIFFERENCE'] != 0:\n",
    "            reasons.append(f\"Original face diff {row['ORIGINALFACEDIFFERENCE']}\")\n",
    "        \n",
    "        if 'TRADE_AGE_DAYS' in row and row['TRADE_AGE_DAYS'] > 7:\n",
    "            reasons.append(f\"Old trade ({row['TRADE_AGE_DAYS']} days)\")\n",
    "        \n",
    "        if 'HAS_ROUNDING_NOTE' in row and 'COMMENT' in row:\n",
    "            if not row['HAS_ROUNDING_NOTE'] and pd.notna(row['COMMENT']):\n",
    "                reasons.append(\"Unusual comment pattern\")\n",
    "        \n",
    "        return \" | \".join(reasons) if reasons else \"Multi-factor anomaly\"\n",
    "\n",
    "    def generate_anomaly_report(self, anomalies):\n",
    "        \"\"\"Create detailed anomaly report\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Generating anomaly report\")\n",
    "            \n",
    "            if anomalies.empty:\n",
    "                logging.info(\"No anomalies detected - empty report\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            report_cols = [\n",
    "                'TRADEID', 'RISKDATE', 'DESKNAME', 'TRADING_UNIT_NAME',\n",
    "                'MatchStatus', 'QUANTITYDIFFERENCE', 'PRICEDIFFERENCE',\n",
    "                'ORIGINALFACEDIFFERENCE', 'Anomaly_Reason', 'COMMENT',\n",
    "                'Anomaly_Score'\n",
    "            ]\n",
    "            report_cols = [col for col in report_cols if col in anomalies.columns]\n",
    "            \n",
    "            report = anomalies[report_cols].sort_values('Anomaly_Score', ascending=True)\n",
    "            \n",
    "            logging.info(\"\\nSample of anomaly report:\")\n",
    "            logging.info(report.head().to_string())\n",
    "            \n",
    "            report.to_csv('detected_anomalies_report.csv', index=False)\n",
    "            logging.info(\"Saved anomaly report to detected_anomalies_report.csv\")\n",
    "            \n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Report generation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def fix_anomalies(self, history_df, anomaly_report):\n",
    "        \"\"\"Fix anomalies with detailed change logging\"\"\"\n",
    "        try:\n",
    "            logging.info(\"\\nStarting anomaly fixing process\")\n",
    "            \n",
    "            fixed_df = history_df.copy()\n",
    "            change_log = []\n",
    "            \n",
    "            if anomaly_report.empty:\n",
    "                logging.info(\"No anomalies to fix\")\n",
    "                return fixed_df\n",
    "            \n",
    "            for _, row in anomaly_report.iterrows():\n",
    "                trade_id = row['TRADEID']\n",
    "                mask = fixed_df['TRADEID'] == trade_id\n",
    "                \n",
    "                if not mask.any():\n",
    "                    logging.warning(f\"Trade ID {trade_id} not found in history data\")\n",
    "                    continue\n",
    "                \n",
    "                original = fixed_df.loc[mask].iloc[0].to_dict()\n",
    "                changes = {\n",
    "                    'TRADEID': trade_id,\n",
    "                    'RISKDATE': row['RISKDATE'],\n",
    "                    'DESKNAME': row['DESKNAME'],\n",
    "                    'ACTION': 'NO_CHANGE',\n",
    "                    'CHANGES': {},\n",
    "                    'REASON': row.get('Anomaly_Reason', 'Unknown reason')\n",
    "                }\n",
    "                \n",
    "                comment = str(row.get('COMMENT', '')).lower()\n",
    "                if 'rounding' in comment:\n",
    "                    changes['ACTION'] = 'ROUNDING_ADJUSTMENT'\n",
    "                    for field in ['QUANTITYDIFFERENCE', 'PRICEDIFFERENCE', 'ORIGINALFACEDIFFERENCE']:\n",
    "                        if field in original and original[field] != 0:\n",
    "                            changes['CHANGES'][field] = {'old': original[field], 'new': 0}\n",
    "                            fixed_df.loc[mask, field] = 0\n",
    "                \n",
    "                elif 'var update' in comment:\n",
    "                    changes['ACTION'] = 'VAR_UPDATE_ADJUSTMENT'\n",
    "                \n",
    "                elif 'blotter code' in comment:\n",
    "                    changes['ACTION'] = 'BLOTTER_CODE_ADJUSTMENT'\n",
    "                \n",
    "                else:\n",
    "                    changes['ACTION'] = 'GENERIC_FIX'\n",
    "                \n",
    "                if 'Anomaly' in original and original['Anomaly']:\n",
    "                    changes['CHANGES']['Anomaly'] = {'old': True, 'new': False}\n",
    "                    fixed_df.loc[mask, 'Anomaly'] = False\n",
    "                \n",
    "                if changes['CHANGES'] or changes['ACTION'] != 'NO_CHANGE':\n",
    "                    change_log.append(changes)\n",
    "                    logging.info(f\"\\n=== Fixing Trade ID: {trade_id} ===\")\n",
    "                    logging.info(f\"Action Type: {changes['ACTION']}\")\n",
    "                    logging.info(f\"Reason: {changes['REASON']}\")\n",
    "                    logging.info(\"Detailed Changes:\")\n",
    "                    for field, change in changes['CHANGES'].items():\n",
    "                        logging.info(f\"  Column: {field:25} Old Value: {change['old']:15} â†’ New Value: {change['new']}\")\n",
    "            \n",
    "            if change_log:\n",
    "                action_counts = pd.DataFrame(change_log)['ACTION'].value_counts()\n",
    "                logging.info(\"\\n=== FIXING SUMMARY ===\")\n",
    "                logging.info(f\"Total anomalies processed: {len(anomaly_report)}\")\n",
    "                logging.info(f\"Total trades modified: {len(change_log)}\")\n",
    "                for action, count in action_counts.items():\n",
    "                    logging.info(f\"  {action}: {count} trades\")\n",
    "                \n",
    "                self._save_change_log(change_log)\n",
    "            else:\n",
    "                logging.info(\"\\nNo changes were made to any trades\")\n",
    "            \n",
    "            fixed_df.to_csv('updated_Catalyst_History_Reconciliation.csv', index=False)\n",
    "            logging.info(\"\\nSaved fixed data to updated_Catalyst_History_Reconciliation.csv\")\n",
    "            \n",
    "            return fixed_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Anomaly fixing failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_change_log(self, change_log):\n",
    "        \"\"\"Save detailed change log with statistics\"\"\"\n",
    "        try:\n",
    "            if not change_log:\n",
    "                logging.info(\"No changes were made - empty change log\")\n",
    "                return\n",
    "                \n",
    "            log_df = pd.DataFrame(change_log)\n",
    "            \n",
    "            changes_expanded = []\n",
    "            for _, row in log_df.iterrows():\n",
    "                base_info = {\n",
    "                    'TRADEID': row['TRADEID'],\n",
    "                    'RISKDATE': row['RISKDATE'],\n",
    "                    'DESKNAME': row['DESKNAME'],\n",
    "                    'ACTION': row['ACTION'],\n",
    "                    'REASON': row['REASON']\n",
    "                }\n",
    "                if row['CHANGES']:\n",
    "                    for field, change in row['CHANGES'].items():\n",
    "                        record = base_info.copy()\n",
    "                        record.update({\n",
    "                            'FIELD': field,\n",
    "                            'OLD_VALUE': change['old'],\n",
    "                            'NEW_VALUE': change['new']\n",
    "                        })\n",
    "                        changes_expanded.append(record)\n",
    "            \n",
    "            summary = {\n",
    "                'total_fixes': len(change_log),\n",
    "                'actions_applied': {},\n",
    "                'fields_modified': {},\n",
    "                'desks_affected': {}\n",
    "            }\n",
    "            \n",
    "            if not log_df.empty:\n",
    "                summary['actions_applied'] = log_df['ACTION'].value_counts().to_dict()\n",
    "                summary['desks_affected'] = log_df['DESKNAME'].value_counts().to_dict()\n",
    "                \n",
    "            if changes_expanded:\n",
    "                changes_df = pd.DataFrame(changes_expanded)\n",
    "                changes_df.to_csv('anomaly_fix_details.csv', index=False)\n",
    "                summary['fields_modified'] = changes_df['FIELD'].value_counts().to_dict()\n",
    "            else:\n",
    "                changes_df = pd.DataFrame()\n",
    "            \n",
    "            logging.info(\"\\n=== DETAILED FIX STATISTICS ===\")\n",
    "            logging.info(f\"Total fixes applied: {summary['total_fixes']}\")\n",
    "            \n",
    "            if summary['actions_applied']:\n",
    "                logging.info(\"\\nActions applied:\")\n",
    "                for action, count in summary['actions_applied'].items():\n",
    "                    logging.info(f\"  {action}: {count}\")\n",
    "                \n",
    "            if summary['fields_modified']:\n",
    "                logging.info(\"\\nFields modified:\")\n",
    "                for field, count in summary['fields_modified'].items():\n",
    "                    logging.info(f\"  {field}: {count}\")\n",
    "                \n",
    "            if summary['desks_affected']:\n",
    "                logging.info(\"\\nDesks affected:\")\n",
    "                for desk, count in summary['desks_affected'].items():\n",
    "                    logging.info(f\"  {desk}: {count}\")\n",
    "            \n",
    "            with open('fix_summary.json', 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not save change log: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def validate_fixes(self, original_df, fixed_df):\n",
    "        \"\"\"Validate that fixes were applied correctly\"\"\"\n",
    "        try:\n",
    "            logging.info(\"\\nValidating fixes...\")\n",
    "            \n",
    "            if len(original_df) != len(fixed_df):\n",
    "                logging.warning(f\"Row count changed: before {len(original_df)}, after {len(fixed_df)}\")\n",
    "            else:\n",
    "                logging.info(\"Row count consistent\")\n",
    "            \n",
    "            original_anomalies = original_df['Anomaly'].sum() if 'Anomaly' in original_df else 0\n",
    "            fixed_anomalies = fixed_df['Anomaly'].sum() if 'Anomaly' in fixed_df else 0\n",
    "            logging.info(f\"Anomaly count: before {original_anomalies}, after {fixed_anomalies}\")\n",
    "            \n",
    "            if 'COMMENT' in fixed_df and 'QUANTITYDIFFERENCE' in fixed_df and 'PRICEDIFFERENCE' in fixed_df:\n",
    "                rounding_fixes = fixed_df[\n",
    "                    (fixed_df['COMMENT'].str.contains('rounding', na=False)) &\n",
    "                    (fixed_df['QUANTITYDIFFERENCE'] == 0) &\n",
    "                    (fixed_df['PRICEDIFFERENCE'] == 0)\n",
    "                ]\n",
    "                logging.info(f\"Applied {len(rounding_fixes)} rounding fixes\")\n",
    "                \n",
    "                if len(rounding_fixes) > 0:\n",
    "                    sample_fix = rounding_fixes.iloc[0]\n",
    "                    logging.info(\"\\nExample rounding fix:\")\n",
    "                    logging.info(f\"TRADEID: {sample_fix['TRADEID']}\")\n",
    "                    logging.info(f\"DESK: {sample_fix['DESKNAME']}\")\n",
    "                    if 'QUANTITYDIFFERENCE' in original_df.columns:\n",
    "                        original_qty = original_df.loc[original_df['TRADEID'] == sample_fix['TRADEID'], 'QUANTITYDIFFERENCE'].values[0]\n",
    "                        logging.info(f\"Qty before: {original_qty}\")\n",
    "                    logging.info(f\"Qty after: 0\")\n",
    "            \n",
    "            logging.info(\"Validation completed\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Validation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Example file paths - replace with your actual paths\n",
    "        history_path = 'Catalyst_History_Reconciliation.csv'\n",
    "        recon_path = 'Catalyst_Reconciliation.csv'\n",
    "        \n",
    "        detector = ReconciliationAnomalyDetector()\n",
    "        \n",
    "        logging.info(\"\\n=== STEP 1: LOADING DATA ===\")\n",
    "        history_df, recon_df, combined_df = detector.load_data(history_path, recon_path)\n",
    "        \n",
    "        logging.info(\"\\n=== STEP 2: PREPROCESSING DATA ===\")\n",
    "        processed_df = detector.preprocess_data(combined_df)\n",
    "        \n",
    "        logging.info(\"\\n=== STEP 3: TRAINING MODEL ===\")\n",
    "        detector.train_model(processed_df)\n",
    "        \n",
    "        logging.info(\"\\n=== STEP 4: DETECTING ANOMALIES ===\")\n",
    "        scored_df, anomalies = detector.detect_anomalies(processed_df)\n",
    "        \n",
    "        logging.info(\"\\n=== STEP 5: GENERATING REPORT ===\")\n",
    "        report = detector.generate_anomaly_report(anomalies)\n",
    "        \n",
    "        logging.info(\"\\n=== STEP 6: FIXING ANOMALIES ===\")\n",
    "        fixed_df = detector.fix_anomalies(history_df, report)\n",
    "        \n",
    "        logging.info(\"\\n=== STEP 7: VALIDATING FIXES ===\")\n",
    "        detector.validate_fixes(history_df, fixed_df)\n",
    "        \n",
    "        logging.info(\"\\n=== PROCESS COMPLETED SUCCESSFULLY ===\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"\\n=== PROCESS FAILED: {str(e)} ===\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOgEHX+bW8rVZqksYLzdty4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
